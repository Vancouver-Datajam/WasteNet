{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zubiamansoor/project_3/blob/master/train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8EqKGRX7hcLr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip3 install torch torchvision"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EqqC-i67jU8Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "torch.cuda.is_available()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "55BlWgJpiu3E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"Headers\"\"\"\n",
        "import os\n",
        "import os.path as osp\n",
        "import time\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "import torch.optim as optim\n",
        "\n",
        "from torchvision import datasets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2djZNjG7tTa5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os.path as osp\n",
        "import os\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "from __future__ import print_function\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import sys\n",
        "if sys.version_info[0] == 2:\n",
        "    import cPickle as pickle\n",
        "else:\n",
        "    import pickle\n",
        "\n",
        "import math\n",
        "\n",
        "from skimage.transform import resize\n",
        "import scipy\n",
        "\n",
        "\n",
        "np.random.seed(111)\n",
        "torch.cuda.manual_seed_all(111)\n",
        "torch.manual_seed(111)\n",
        "\n",
        "\n",
        "class Dataset_Gary(Dataset):\n",
        "\n",
        "\tdef __init__(self, root, fold=\"train\",\n",
        "\t\t\t\t transform=None, target_transform=None):\n",
        "\t\t\n",
        "\t\tfold = fold.lower()\n",
        "\n",
        "\t\tself.train = False\n",
        "\t\tself.test = False\n",
        "\t\tself.val = False\n",
        "\n",
        "\t\tif fold == \"train\":\n",
        "\t\t\tself.train = True\n",
        "\t\telif fold == \"test\":\n",
        "\t\t\tself.test = True\n",
        "\t\telif fold == \"val\":\n",
        "\t\t\tself.val = True\n",
        "\t\telse:\n",
        "\t\t\traise RuntimeError(\"Not train-val-test\")\n",
        "\n",
        "\n",
        "\t\tself.root = os.path.expanduser(root)\n",
        "\t\tself.transform = transform\n",
        "\t\tself.target_transform = target_transform\n",
        "\n",
        "\t\tfpath = self.root\n",
        "\n",
        "\t\t# now load the picked numpy arrays\n",
        "\t\tself.data = []\n",
        "\t\tif self.train:\n",
        "\t\t\tself.datalist_dir = os.path.join(self.root, 'train_list.txt')\n",
        "\t\tif self.val:\n",
        "\t\t\tself.datalist_dir = os.path.join(self.root, 'val_list.txt')\n",
        "\t\tif self.test:\n",
        "\t\t\tself.datalist_dir = os.path.join(self.root, 'test_list.txt')\n",
        "\n",
        "\t\twith open(self.datalist_dir, 'r') as f:\n",
        "\t\t\tfor line in f:\n",
        "\t\t\t\tif line[0] == '#' or len(line.strip()) == 0:\n",
        "\t\t\t\t\tcontinue\n",
        "\t\t\t\tparams = line.strip().split()\n",
        "\t\t\t\tself.data.append({\n",
        "\t\t\t\t\t'file_name' : params[0],\n",
        "\t\t\t\t\t'label' : params[1],})\n",
        "\n",
        "\tdef __getitem__(self, index):\n",
        "\t\tlabel = self.data[index]['label']\n",
        "\t\tif label == 'cardboard':\n",
        "\t\t\ttarget = 0\n",
        "\t\tif label == 'glass':\n",
        "\t\t\ttarget = 1\n",
        "\t\tif label == 'metal':\n",
        "\t\t\ttarget = 2\n",
        "\t\tif label == 'paper':\n",
        "\t\t\ttarget = 3\n",
        "\t\tif label == 'plastic':\n",
        "\t\t\ttarget = 4\n",
        "\t\tif label == 'trash':\n",
        "\t\t\ttarget = 5\n",
        "\t\timg = plt.imread(osp.join(self.root, self.data[index]['label'], self.data[index]['file_name']))\n",
        "\n",
        "\t\t# doing this so that it is consistent with all other datasets\n",
        "\t\t# to return a PIL Image\n",
        "\t\timg = Image.fromarray(img)\n",
        "\n",
        "\t\tif self.transform is not None:\n",
        "\t\t\timg = self.transform(img)\n",
        "\n",
        "\t\tif self.target_transform is not None:\n",
        "\t\t\ttarget = self.target_transform(target)\n",
        "\n",
        "\t\treturn img, target\n",
        "\n",
        "\tdef __len__(self):\n",
        "\t\treturn len(self.data)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HUdo6AkH0maX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PreTrainedResNet(nn.Module):\n",
        "  def __init__(self, num_classes, feature_extracting):\n",
        "    super(PreTrainedResNet, self).__init__()\n",
        "    \n",
        "    self.resnet18 = models.resnet18(pretrained=True)\n",
        "\n",
        "    if feature_extracting:\n",
        "      for param in self.resnet18.parameters():\n",
        "          param.requires_grad = False\n",
        "    \n",
        "    num_feats = self.resnet18.fc.in_features\n",
        "    \n",
        "    self.resnet18.fc =  nn.Linear(num_feats,num_classes)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.resnet18.forward(x)\n",
        "    return x\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ujUNEVsEvWwv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, optimizer, criterion, epoch, num_epochs):\n",
        "  model.train()\n",
        "  epoch_loss = 0.0\n",
        "  epoch_acc = 0.0\n",
        "  \n",
        "  for batch_idx, (images, labels) in enumerate(dataloaders['train']):\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    images, labels = images.cuda(), labels.cuda()\n",
        "    \n",
        "    outputs = model.forward(images)\n",
        "    \n",
        "    loss = criterion(outputs, labels)\n",
        "    \n",
        "    _, preds = torch.max(outputs.data, 1)\n",
        "    \n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    epoch_loss += loss.item()\n",
        "    epoch_acc += torch.sum(preds == labels).item()\n",
        "    \n",
        "  epoch_loss /= dataset_sizes['train']\n",
        "  epoch_acc /= dataset_sizes['train']\n",
        "  \n",
        "  print('TRAINING Epoch %d/%d Loss %.4f Accuracy %.4f' % (epoch, num_epochs, epoch_loss, epoch_acc))\n",
        "\n",
        "  return epoch_loss, epoch_acc\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Md7WhEJ_1XGS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sklearn.metrics as metric\n",
        "\n",
        "def test(model, criterion, repeats=2):\n",
        "  model.eval()\n",
        "  \n",
        "  test_loss = 0.0\n",
        "  test_acc = 0.0\n",
        "  f1_score = 0.0\n",
        "  f1_score_w = 0.0\n",
        "  conf_mat = np.zeros([len(class_names),len(class_names)])\n",
        "  with torch.no_grad():\n",
        "    for itr in range(repeats):\n",
        "      for batch_idx, (images, labels) in enumerate(dataloaders['test']):\n",
        "        #move to GPU\n",
        "        images, labels = images.cuda(), labels.cuda()\n",
        "\n",
        "        #forward\n",
        "        outputs = model.forward(images)\n",
        "        _, preds = torch.max(outputs.data, 1)\n",
        "\n",
        "        predlabels = preds.cpu().numpy()\n",
        "        labels_num = labels.cpu().numpy()\n",
        "        for ind,label in enumerate(labels_num):\n",
        "          conf_mat[label,predlabels[ind]] = conf_mat[label,predlabels[ind]] + 1\n",
        "        \n",
        "\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        _, preds = torch.max(outputs.data, 1)\n",
        "\n",
        "        test_loss += loss.item()\n",
        "        test_acc += torch.sum(preds == labels).item()\n",
        "          \n",
        "        f1_score += metric.f1_score(labels_num, predlabels,labels=[0,1,2,3,4,5], average='weighted', zero_division='warn')\n",
        "        f1_score_w += metric.f1_score(labels_num, predlabels,labels=[0,1,2,3,4,5], average='macro', zero_division='warn')\n",
        "\n",
        "    test_loss /= (dataset_sizes['test']*repeats)\n",
        "    test_acc /= (dataset_sizes['test']*repeats)\n",
        "    f1_score /= (dataset_sizes['test']*repeats)\n",
        "\n",
        "\n",
        "    print('Test Loss: %.4f Test Accuracy %.4f Weighted: %.4f Macro: %.4f' % (test_loss, test_acc, f1_score, f1_score_w))\n",
        "    return test_loss, test_acc, conf_mat\n",
        "\n",
        "\n",
        "def val(model, criterion, repeats=2):\n",
        "  model.eval()\n",
        "  \n",
        "  test_loss = 0.0\n",
        "  test_acc = 0.0\n",
        "  \n",
        "  with torch.no_grad():\n",
        "    for itr in range(repeats):\n",
        "      for batch_idx, (images, labels) in enumerate(dataloaders['val']):\n",
        "        #move to GPU\n",
        "        images, labels = images.cuda(), labels.cuda()\n",
        "\n",
        "        #forward\n",
        "        outputs = model.forward(images)\n",
        "\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        _, preds = torch.max(outputs.data, 1)\n",
        "\n",
        "        test_loss += loss.item()\n",
        "        test_acc += torch.sum(preds == labels).item()\n",
        "\n",
        "    test_loss /= (dataset_sizes['val']*repeats)\n",
        "    test_acc /= (dataset_sizes['val']*repeats)\n",
        "\n",
        "    print('Val Loss: %.4f Val Accuracy %.4f' % (test_loss, test_acc))\n",
        "\n",
        "    return test_loss, test_acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OsmSIWL8ovUw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "NUM_EPOCHS = 25\n",
        "LEARNING_RATE = 0.001 \n",
        "BATCH_SIZE = 10\n",
        "RESNET_LAST_ONLY = False #Fine tunes only the last layer. Set to False to fine tune entire network\n",
        "\n",
        "root_path = 'dataset/'\n",
        "\n",
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.Resize(384),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomVerticalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    ]),\n",
        "    'test': transforms.Compose([\n",
        "        transforms.Resize(384),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    ]),\n",
        "}\n",
        "\n",
        "# loading datasets with PyTorch ImageFolder\n",
        "image_datasets_train = Dataset_Gary(root_path, fold=\"train\",\n",
        "\t\t\t\t transform=data_transforms['train'], target_transform=None)\n",
        "\n",
        "image_datasets_val = Dataset_Gary(root_path, fold=\"val\",\n",
        "\t\t\t\t transform=data_transforms['test'], target_transform=None)\n",
        "\n",
        "image_datasets_test = Dataset_Gary(root_path, fold=\"test\",\n",
        "\t\t\t\t transform=data_transforms['test'], target_transform=None)\n",
        "\n",
        "# defining data loaders to load data using image_datasets and transforms, here we also specify batch size for the mini batch\n",
        "\n",
        "dataloader_train = torch.utils.data.DataLoader(image_datasets_train, batch_size=BATCH_SIZE,\n",
        "                                             shuffle=True, num_workers=4)\n",
        "            \n",
        "dataloader_val = torch.utils.data.DataLoader(image_datasets_val, batch_size=BATCH_SIZE,\n",
        "                                             shuffle=True, num_workers=4)\n",
        "\n",
        "dataloader_test = torch.utils.data.DataLoader(image_datasets_test, batch_size=BATCH_SIZE,\n",
        "                                             shuffle=True, num_workers=4)\n",
        "\n",
        "dataloaders = {'train': dataloader_train, 'test': dataloader_test, 'val':dataloader_val}\n",
        "\n",
        "dataset_size_train = len(image_datasets_train)\n",
        "dataset_size_val = len(image_datasets_val)\n",
        "dataset_size_test = len(image_datasets_test)\n",
        "\n",
        "dataset_sizes = {'train': dataset_size_train, 'test': dataset_size_test, 'val':dataset_size_val}\n",
        "\n",
        "class_names = ['cardboard', 'glass', 'metal', 'paper', 'plastic', 'trash']\n",
        "\n",
        "#Initialize the model\n",
        "model = PreTrainedResNet(len(class_names), RESNET_LAST_ONLY)\n",
        "model = model.cuda()\n",
        "\n",
        "#Setting the optimizer and loss criterion\n",
        "optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=0.9 , weight_decay=1e-3)\n",
        "\n",
        "weightlist = [1,1,1,1,1,4]\n",
        "weightlist = torch.Tensor(weightlist)\n",
        "weightlist = weightlist.cuda()\n",
        "criterion = nn.CrossEntropyLoss(weight = weightlist)\n",
        "\n",
        "train_loss_list =[]\n",
        "train_acc_list = []\n",
        "val_loss_list =[]\n",
        "val_acc_list = []\n",
        "\n",
        "#Begin Train\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  t1,t2 = train(model, optimizer, criterion, epoch+1, NUM_EPOCHS)\n",
        "  train_loss_list.append(t1)\n",
        "  train_acc_list.append(t2)\n",
        "  if (epoch+1) % 5 == 0:\n",
        "    t1,t2 = val(model, criterion)\n",
        "    val_loss_list.append(t1)\n",
        "    val_acc_list.append(t2)\n",
        "  \n",
        "print(\"Finished Training\")\n",
        "print(\"-\"*10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fv9IOqoXozir",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "t1,t2,conf_mat = test(model, criterion)\n",
        "print('Conf Mat\\n',conf_mat)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rji12O1KTumN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(6):\n",
        "  conf_mat[i,:] = conf_mat[i,:]/sum(conf_mat[i,:])\n",
        "\n",
        "plt.imshow(conf_mat, cmap='hot')\n",
        "\n",
        "plt.xticks([0,1,2,3,4,5],class_names)\n",
        "plt.yticks([0,1,2,3,4,5],class_names)\n",
        "plt.colorbar()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zd_lkTdoUaOX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def imshow(inp, title=None):\n",
        "    \"\"\"Imshow for Tensor.\"\"\"\n",
        "    inp = inp.numpy().transpose((1, 2, 0))\n",
        "    inp = np.clip(inp, 0, 1)\n",
        "    plt.axis(\"off\")\n",
        "    plt.imshow(inp)\n",
        "    if title is not None:\n",
        "        plt.title(title)\n",
        "    plt.pause(1)  # pause a bit so that plots are updated\n",
        "    \n",
        "def visualize_model(model, num_images=8):\n",
        "    images_so_far = 0\n",
        "    fig = plt.figure()\n",
        "\n",
        "    for batch_idx, (images, labels) in enumerate(dataloaders['test']):\n",
        "        #move to GPU\n",
        "        images, labels = images.cuda(), labels.cuda()\n",
        "        \n",
        "        outputs = model(images)\n",
        "        \n",
        "        _, preds = torch.max(outputs.data, 1)\n",
        "       \n",
        "\n",
        "        for j in range(images.size()[0]):\n",
        "            # if preds[j] == labels[j]:\n",
        "            #   continue \n",
        "            images_so_far += 1\n",
        "            #ax = plt.subplot(num_images//2, 2, images_so_far)\n",
        "  \n",
        "            #plt.axis('off')\n",
        "            #ax.set_title('class: {} predicted: {}'.format(class_names[labels.data[j]], class_names[preds[j]]))\n",
        "            print('class: {} predicted: {}'.format(class_names[labels.data[j]], class_names[preds[j]]))\n",
        "            imshow(images.cpu().data[j])\n",
        "\n",
        "            if images_so_far == num_images:\n",
        "              return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WUYa3X4ZrCqk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "visualize_model(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "irbulScrVkRi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from scipy import interpolate\n",
        "x = np.arange(5, 25)\n",
        "x_train = np.arange(0,25)\n",
        "x1 = [5, 10, 15, 20, 25]\n",
        "f_loss = interpolate.interp1d(x1, val_loss_list)\n",
        "f_accuracy = interpolate.interp1d(x1, val_acc_list)\n",
        "\n",
        "\n",
        "val_acc = f_accuracy(x)   # use interpolation function returned by `interp1d`\n",
        "val_loss = f_loss(x)\n",
        "\n",
        "train_loss_list\n",
        "train_acc_list\n",
        "val_loss\n",
        "val_acc\n",
        "\n",
        "\n",
        "plt.plot(x_train, train_loss_list)\n",
        "plt.plot(x, val_loss)\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'])\n",
        "plt.show()\n",
        "plt.figure()\n",
        "plt.plot(x_train, train_acc_list)\n",
        "plt.plot(x, val_acc)\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dyDPGRCgtfth",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def test_new(dataloader_new, model, criterion, repeats=2):\n",
        "#   model.eval()\n",
        "  \n",
        "#   test_loss = 0.0\n",
        "#   test_acc = 0.0\n",
        "#   f1_score = 0.0\n",
        "#   f1_score_w = 0.0\n",
        "#   conf_mat = np.zeros([len(class_names),len(class_names)])\n",
        "#   with torch.no_grad():\n",
        "#     for itr in range(repeats):\n",
        "#       for batch_idx, (images, labels) in enumerate(dataloader_new):\n",
        "#         #move to GPU\n",
        "#         images, labels = images.cuda(), labels.cuda()\n",
        "#         #print(images.shape())\n",
        "\n",
        "#         #forward\n",
        "#         outputs = model.forward(images)\n",
        "#         _, preds = torch.max(outputs.data, 1)\n",
        "\n",
        "#         predlabels = preds.cpu().numpy()\n",
        "#         labels_num = labels.cpu().numpy()\n",
        "#         for ind,label in enumerate(labels_num):\n",
        "#           conf_mat[label,predlabels[ind]] = conf_mat[label,predlabels[ind]] + 1\n",
        "        \n",
        "\n",
        "#         loss = criterion(outputs, labels)\n",
        "\n",
        "#         _, preds = torch.max(outputs.data, 1)\n",
        "\n",
        "#         test_loss += loss.item()\n",
        "#         test_acc += torch.sum(preds == labels).item()\n",
        "          \n",
        "#         f1_score += metric.f1_score(labels_num, predlabels,labels=[0,1,2,3,4,5], average='weighted', zero_division='warn')\n",
        "#         f1_score_w += metric.f1_score(labels_num, predlabels,labels=[0,1,2,3,4,5], average='macro', zero_division='warn')\n",
        "\n",
        "#     test_loss /= (dataset_sizes['test']*repeats)\n",
        "#     test_acc /= (dataset_sizes['test']*repeats)\n",
        "#     f1_score /= (dataset_sizes['test']*repeats)\n",
        "\n",
        "\n",
        "#     print('Test Loss: %.4f Test Accuracy %.4f Weighted: %.4f Macro: %.4f' % (test_loss, test_acc, f1_score, f1_score_w))\n",
        "#     return test_loss, test_acc, conf_mat\n",
        "\n",
        "# root_path_new = 'our_dataset/' #If your data is in a different folder, set the path accodordingly\n",
        "\n",
        "# new_dataset_test = Dataset_Gary(root_path_new, fold=\"test\",\n",
        "# \t\t\t\t transform=transforms.Compose([\n",
        "#         transforms.Resize((384, 512)),\n",
        "#         transforms.ToTensor(),\n",
        "#         transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "#     ]), target_transform=None)\n",
        "\n",
        "# # defining data loaders to load data using image_datasets and transforms, here we also specify batch size for the mini batch\n",
        "\n",
        "# dataloader_test_new = torch.utils.data.DataLoader(new_dataset_test, batch_size=BATCH_SIZE,\n",
        "#                                              shuffle=True, num_workers=4)\n",
        "\n",
        "# dataloaders = {'train': dataloader_train, 'test': dataloader_test_new, 'val':dataloader_val}\n",
        "\n",
        "# dataset_size_train = len(image_datasets_train)\n",
        "# dataset_size_val = len(image_datasets_val)\n",
        "# dataset_size_test_new = len(new_dataset_test)\n",
        "\n",
        "# dataset_sizes = {'train': dataset_size_train, 'test': dataset_size_test_new, 'val':dataset_size_val}\n",
        "\n",
        "# t1_new,t2_new,conf_mat_new = test_new(dataloader_test_new,model, criterion)\n",
        "# print('Conf Mat\\n',conf_mat_new)\n",
        "\n",
        "# def imshow(inp, title=None):\n",
        "#     \"\"\"Imshow for Tensor.\"\"\"\n",
        "#     inp = inp.numpy().transpose((1, 2, 0))\n",
        "#     inp = np.clip(inp, 0, 1)\n",
        "#     plt.axis(\"off\")\n",
        "#     plt.imshow(inp)\n",
        "#     if title is not None:\n",
        "#         plt.title(title)\n",
        "#     plt.pause(1)  # pause a bit so that plots are updated\n",
        "    \n",
        "# def visualize_model_new(dataloader, model, num_images=8):\n",
        "#     images_so_far = 0\n",
        "#     fig = plt.figure()\n",
        "\n",
        "#     for batch_idx, (images, labels) in enumerate(dataloader):\n",
        "#         #move to GPU\n",
        "#         images, labels = images.cuda(), labels.cuda()\n",
        "        \n",
        "#         outputs = model(images)\n",
        "        \n",
        "#         _, preds = torch.max(outputs.data, 1)\n",
        "       \n",
        "\n",
        "#         for j in range(images.size()[0]):\n",
        "#             #if preds[j] == labels[j]:\n",
        "#              # continue \n",
        "#             images_so_far += 1\n",
        "#             #ax = plt.subplot(num_images//2, 2, images_so_far)\n",
        "  \n",
        "#             #plt.axis('off')\n",
        "#             #ax.set_title('class: {} predicted: {}'.format(class_names[labels.data[j]], class_names[preds[j]]))\n",
        "#             print('class: {} predicted: {}'.format(class_names[labels.data[j]], class_names[preds[j]]))\n",
        "#             imshow(images.cpu().data[j])\n",
        "\n",
        "#             if images_so_far ==20:\n",
        "#               return\n",
        "# visualize_model_new(dataloader_test_new,model)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}